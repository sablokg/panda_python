# Understanding DataFrames in Pandas

Defining a data frame using pandas and to check the version of the installed pandas from heapq import merge
```
from msilib.schema import ServiceInstall, tables
from pickle import TRUE
from tkinter import Frame
from turtle import right
import pandas as pd && print(pd.__version__)
```

Here the dataframe is defined as Dataframe.read_csv and in the example below Dataframe is named as pd
Indexing the Datasets and printing the dataframe
```
import pandas as pd (importing pandas)
df = pd.read_csv('data.csv')
df
index_df = df.index
columns = df.columns
values = df.values
```
Another function to import the csv data into the panda using the defined delimiter
```
import pandas as pd
> df = pd.read_table("filename.csv", delimiter =", ")
print(df.head())
```
Checking type of index_df, columns, values
```
print (type(df_index)) or print (type(columns)) or print(type(values))
```
You can index a column by index= ['columnname'] or by simply appending it to the index such as df.cloumnname. This is the same as you use in the dictionary for identifying the key value pair. You can also make new indexes by making a specific columns as an index of a dataframe such as 

Reading data from excel file and checking the sheets and reading a specific sheet
```
df_excel = pd.read_excel(r'~/Desktop/file.xlsx') # read Excel file into a DataFrame
xl.sheet_names
df_excel = pd.read_excel(r'~/Desktop/file.xlsx', sheet_name= 'sheetname')
```

Reading a JSON file and making a data frame. In case of the big data, most of the data is stored in the form of the JSON, so to import the JSON data into the data frame
```
import pandas as pd
df = pd.read_json('data.json')
print(df.to_string()) # This will print entire data frame
```
Print head of DataFrame df
```
df
```
Reading a dataframe from clipboard 
```
df = pd.read_clipboard()
```

Paralleisation on DataFrames
Pandarallel allows you to parallelize its operations to multiple CPU cores - by changing just one line of code. 
Supported methods include apply(), applymap(), groupby(), map() and rolling().



You can also import the pandas dataframe as datatable specific
```
file =  'df.csv'
import datatable as dt 
df.final = df.fread(df)
df= df.to_pandas()
```

Performing mathematical calculations on columns
```
import pandas as pd (importing pandas)
df = pd.read_csv('data.csv')
columnname= d['columname']
columname
```
Now you can perform all the arthimetic on this column alone such as +, -, *, /, //, %, **
```
colunname.value_counts()
columnname.value_counts(normalize=True)
columname + 3
columname * 3
columname - 3
columname / 3
columname // 3
columname ** 3
```
Checking the number of rows and columns in dataframe
This will display the number of the rows in the dataframe and by default it will display only 60 rows and the print (df) will return only the first and the last 5 rows.
```
import pandas as pd (importing pandas)
print(pd.options.display.max_rows)
```  
To change this and make the number of the rows to the maximum for the display.
```
import pandas as pd (importing pandas)
pd.options.display.max_rows = 9999
```

Visualisation of the data imported 
```
import pandas as pd
df = pd.read_csv('data.csv')
This will show the first 10 head lines of the data if the number is not specified such as df.head() then it will shows the first 5 lines of the data frame. 
print(df.head(10)) 
This will show the first 10 tail lines of the data
print(df.tail(10))  
```

Estimating the number of the rows and columns also called as shape in pandas data frame
```
import pandas as pd
df = pd.read_csv('data.csv')
df.head() # viewing the first few lines of the data frame
df.shape() # counting the summary or the shape of the data frame
df ['columnname'].count # counting the column numbers
```

Merge, Join and Concatenate data frames
left join = Use keys from left frame 
Right join = Use keys from right frame
Outer join = Use union of keys from both frames
inner join = Use intersection of keys from both frames 
```
import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second dataframe
```
Merging of the dataframe has only few options to consider, most of which is focussed on left join, right join, outer join and inner join. 

Case 1: Where both the dataframes have the same columns id
```
df.inner = pd.merge(df1,df2, how = 'inner', on= 'column name') # in this case, by default the how is set to inner, so only those cases will be merged which are at intersection.
 ```   
Case 2: Where both the datasets have unequal list and you want to merge all the datasets into one tables
```
df.left = pd.merged (df1, df2, how='left', left_on = 'column name', right on = 'column name') This will keep the dataframe of the left intact and will incorporate all the elements of the right dataset.
 ```    
Case 3: Where both the datasets have unqeual list and you want to merge all the datasets into one tables
```
df.right = pd.merged (df1, df2, how = 'right', left_on = 'column name', right on = 'column name') This will return all the values in the left that matches to the right frame. So, this is exactly similiar to the inner join)
```
Case 4: When you have the datasets with the unequal list and you want to merge all of them whether they are present or absent in either of them
```
df.outer = pd.merge(df2, df1, how='outer', indicator=True) # Here the unique call is the indicator= True which will add a new columns called as merge to show which are present in which table
```     
Now lets dig the join and join is a method of the data frame, so it can be call as 
```
import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second dataframe
```

This is telling that join df1 with df2 and keep the data frame df2 to the right.
```
df.merged = df1.join(df2, rsuffix=right)

In this case, it might be possible that you have the duplicated ids, so to avoid the duplicates ids, you can simply index the dataframes such as below.In this we are indexing the column that you have the same tags and avoiding the duplicates.
```
df.merged = df1.set_inex ('column name').join (df2.set_index('column name'))  
```
Append and concatentation fucntions are called as the functions of the dataframe means it will require 2 variables to read append (x,y) or concat (x,y) similar to the join (x,y)
Append function will not filter anything in the tables, it will simply append the tables, it is similar to the PASTE function of the linux. 
```
df.append = df1.append(df2, ignore_index=True) 
```

Concat is the same as merging two dataframes. Lets start with this 
```
import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second dataframe
```

Both the datsets have unqueal columns so to get all the columns 
This will give everything including the duplicated ones. Now to remove the duplicated ones, we can index it.
```
df.concat = pd.concat([df1, df2]) 
```
You can do this also in the same function like given below
```
df_no_duplicates = df.concat.reset_index(drop=True),
```
Concatentate both the indexes and drop the duplicates.
```
df.concat = pd.concat ([df1, df2], ignore_index=TRUE)  
```
With axis= 1 we are defining the column on which the concat needs to be performed. 
```
df.concat = pd.concat([df1, df2], axis= 1)  
```

Point of Note: Merge and Join use key mapping where as Concat doesnt. Concat works both horizontally and vertically. 

Updating the table with another table: In this you can use two functions such as combine_first() and update()
```
import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second datafram
df_new = df1.combine_first(df2) # This will replace the missing values in the df1 from df2. 
df_new = df1.update(df2, overwrite= TRUE) # This will update the values in the df1 from df2 and will overwrite the values.  
df_new = df1.update(df2, overwrite= FALSE) # This will update the values in the df1 from df2 and will not overwrite the values.  
```

Making dataframes from PDF files: There is a library for the extraction of the tables from the PDFs and the library is called as tabula
pip install tabula # for the installation of the library. A similar function also present in R : https://cran.r-hub.io/web/packages/tabulizer/vignettes/tabulizer.html
```
import tabula
myfile = 'filename.pdf'
mytable = tabula.read_pdf(myfile, pages = 1, multiple_tables = TRUE) # Extract the tables from the PDF and keep them as independent tables
```
import tabula
myfile = 'filename.pdf'
mytable = tabula.read_pdf(myfile, pages = 1, multiple_tables = FALSE) # Extract the tables from the PDF and make a single table
tabula.convert_into(myfile, "file.csv") # export the table into the CSV format
```
import tabula
myfile = 'filename.pdf'
mytable = tabula.read_pdf(myfile, pages = 1, multiple_tables = FALSE)# Extract the tables from the PDF and make a single table
tabula.convert_into(myfile, "file.csv") # export the table into the CSV format
```
Reading the html files and making a Dataframe
```
html_df = pd.read_html(str(tbl))[0]\[['Columnname1','Columname2']]
```
x = df['Columnname1'].values.reshape(num_records, 1)
y = df['Columnname2'].values.reshape(num_records, 1)
```
To get the summary of the data frame as to see which are the null values and how much and what kind of dataset you have, you can use two commands 
which will give information and will describe your data frame. 
```
import pandas as pd
df1 = pd.read_csv('data.csv')
df1.describe()
df1.info()
```
First thirty rows of the data frame, now see that a slice function has been used to list [start:stop].
df1[:30]  
```
Changing the type pattern in the python pandas 
```
import pandas as pd
df1 = pd.read_csv('data.csv')
df1.describe()
df1.info()
```
Basic concept of type pattern 
```
Here i have applied the str type to the entire data series 
```
df1 = df1.astype(str)
```
Here i have applied to a specific column of the series
```
df1 = df1.astype ({'colunname: type'})
df1 ['Columnname'] = df1['Columname'].astype ('int or str or object or category')  
```   
Checking the type of the column,so here i am asking the type of the column name in the df1.
```
type(df1.columnname[]) 
``` 
Changing the column type to int64. Remember .astype also accepts dictionary so you can pass multiple columns and data types accordingly.
``` 
df1.cloumnname = df1.columnname.astype ('init64')
df1 = df.astype({"Column1":'type', "Column2":'type'})
```

Applying pandas.to_numeric, pandas.to_datetime, and pandas.to_timedelta
```
df1 = df.apply (pd.to_numeric) # this will convert the complete data frame to numeric. However, if you want to apply this to only some columns then make a list of those columns such as
df1 = df [['Columnname1', 'Columnname2']].apply (pd.to_numeric)
```
Another way to check the data types is using .dtypes
``` 
import pandas as pd
df1 = pd.read_csv('data.csv')
df1.describe()
df1.size()
df1.shape()
df1.info()
df1.dtypes
df.get_dtype_counts() to count each type of the data present in the dataframe
```
                                  
