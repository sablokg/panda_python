Understanding data frames in Panda 
Practicing datasets: https://docs.openml.org/Datasets/


1. Defining a data frame using pandas and to check the version of the installed pandas from heapq import merge
from msilib.schema import ServiceInstall, tables
from pickle import TRUE
from tkinter import Frame
from turtle import right
import pandas as pd && print(pd.__version__)

############ Here the dataframe is defined as Dataframe.read_csv and in the example below Dataframe is named as pd #######

################### some checks #####################

----->this will display the number of the rows in the dataframe and by default it will display only 60 rows and the print (df) will return only the first and the last 5 rows.
import pandas as pd (importing pandas)
print(pd.options.display.max_rows)  
----->To change this and make the number of the rows to the maximum for the display.
import pandas as pd (importing pandas)
pd.options.display.max_rows = 9999

------> In case of the big data, most of the data is stored in the form of the JSON, so to import the JSON data into the data frame 
import pandas as pd
df = pd.read_json('data.json')
print(df.to_string())

------> Visualisation of the data imported 
import pandas as pd
df = pd.read_csv('data.csv')
print(df.head(10)) #### This will show the first 10 head lines of the data if the number is not specified such as df.head() then it will shows the first 5 lines of the data frame. 
print(df.tail(10)) #### This will show the first 10 tail lines of the data  
****** Another function to import the csv data into the panda using the defined delimiter 
import pandas as pd
> df = pd.read_table("filename.csv", delimiter =", ")
print(df.head())

#############################################################################################
-----> Before working with the statistical programming for the data science, you might want to clean the bad data such as Empty cells, Data in wrong format, Wrong data, Duplicates
-----> Removing empty cells as the empty should not be a part of the analysis 
import pandas as pd
df = pd.read_csv('data.csv')
new_df = df.dropna() ##### Return a new Data Frame with no empty cells
print(new_df.to_string())
-----> If you want to change the original DataFrame
import pandas as pd
df = pd.read_csv('data.csv')
df.dropna(inplace = True)
print(df.to_string())
-----> Another way of dealing with the empty cells is to insert a value into the empty cell. 
import pandas as pd
df = pd.read_csv('data.csv')
df.fillna(130, inplace = True) ##### In this a new value 130 is inserted into the same data frame. 
-----> If you want to replace the empty values in a specified column 
import pandas as pd
df = pd.read_csv('data.csv')
df["ColumnName"].fillna(130, inplace = True) #### Here ColumnName is the name of the column and 130 is the value which should be inserted and since it is inplace=TRUE means that do it in the same column. 
-----> Another way of replacing the empty cells in the column with median, mean and mode, 
----------------> Mean = the average value (the sum of all values divided by number of values 
import pandas as pd
df = pd.read_csv('data.csv')
x = df["ColumnName"].mean()
df["ColumnName"].fillna(x, inplace = True)
---------------------> Median = the value in the middle, after you have sorted all values ascending
import pandas as pd
df = pd.read_csv('data.csv')
x = df["ColumnName"].median()
df["ColumnName"].fillna(x, inplace = True)
------------------------------> Mode = the value that appears most frequently
import pandas as pd
df = pd.read_csv('data.csv')
x = df["Calories"].mode()[0] #######[0] in the end means that the column is indexed as first
df["Calories"].fillna(x, inplace = True)
------> Before moving forward, if you have a wrong format, it should be fixed such as 
     Duration          Date  Pulse  Maxpulse  Calories
  0         60  '2020/12/01'    110       130     409.1
  1         60  '2020/12/02'    117       145     479.0
  2         60  '2020/12/03'    103       135     340.0  ###### Check out the date in the row 4 and 5 are not correct. 
  3         45  '2020/12/04'    109       175     282.4
  4        45           NaN    100       119     282.0
  5        60      20201226    100       120     250.0

import pandas as pd
df = pd.read_csv('data.csv')
df['Date'] = pd.to_datetime(df['Date'])
print(df.to_string())
------> after the implication, the revised table is 
  0         60  '2020/12/01'    110       130     409.1
  1         60  '2020/12/02'    117       145     479.0
  2         60  '2020/12/03'    103       135     340.0  ###### Check out the date in the row 4 shows a NA value and the 5 is corrected. 
  3         45  '2020/12/04'    109       175     282.4
  4        45           NaT    100       119     282.0
  5        60   '2020/12/26'    100       120     250.0
----------------> Now in this case, the NA value should be dropped such as 
df.dropna(subset=['Date'], inplace = True)
--------------------------------> Correcting the number format 
see the table below and this table has one number incorrect as this is not the pattern followed 
     Duration          Date  Pulse  Maxpulse  Calories
  0         60  '2020/12/01'    110       130     409.1
  1         60  '2020/12/02'    117       145     479.0
  2         60  '2020/12/03'    103       135     340.0
  3         45  '2020/12/04'    109       175     282.4
  4         45  '2020/12/05'    117       148     406.0
  5         60  '2020/12/06'    102       127     300.0
  6         60  '2020/12/07'    110       136     374.0
  7        450  '2020/12/08'    104       134     253.3 ###### This number 450 is different from the rest of the number. 
----------------> Replacing the value 
df.loc[7, 'Duration'] = 45 in this the number 450 is replaced by 45, at position 7. 
---------------->  In case of large datasets, you can loop through the multiple values. 
for x in df.index: if df.loc[x, "Duration"] > 120: df.loc[x, "Duration"] = 120 suggesting that loop through the duration column and if the value is higher than 120, set it to 120:
-------------------> In case of large datasets, you can also drop those rows and do through the loop 
for x in df.index: if df.loc[x, "Duration"] > 120: df.drop(x, inplace = True) suggesting that loop through the duration column and where the value is greater than 120, drop those rows. 
-------------------------------------> Removing the duplicated values 
print(df.duplicated()) will return the duplicated items and 
df.drop_duplicates(inplace = True) will remove all the duplicates and with in the data frame and will not give a new data frame.

# Estimating the number of the rows and columns also called as shape in pandas data frame

import pandas as pd
df = pd.read_csv('data.csv')
df.head() # viewing the first few lines of the data frame
df.shape() # counting the summary or the shape of the data frame

# Merge, Join and Concatenate data frames
# left join = Use keys from left frame 
# Right join = Use keys from right frame
# Outer join = Use union of keys from both frames
# inner join = Use intersection of keys from both frames 

import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second dataframe

    #Merging of the dataframe has only few options to consider, most of which is focussed on left join, right join, outer join and inner join. 
    # 1. Case 1: Where both the dataframes have the same columns id
    df.inner = pd.merge(df1,df2, how = 'inner', on= 'column name') # in this case, by default the how is set to inner, so only those cases will be merged which are at intersection.
    
    # 2. Case 2: Where both the datasets have unequal list and you want to merge all the datasets into one tables
     df.left = pd.merged (df1, df2, how='left', left_on = 'column name', right on = 'column name') This will keep the dataframe of the left intact and will incorporate all the elements of the right dataset.
     
    # 3. Case 3: Where both the datasets have unqeual list and you want to merge all the datasets into one tables
     df.right = pd.merged (df1, df2, how = 'right', left_on = 'column name', right on = 'column name') This will return all the values in the left that matches to the right frame. So, this is exactly similiar to the inner join)

    # 4. Case 4: When you have the datasets with the unequal list and you want to merge all of them whether they are present or absent in either of them
      df.outer = pd.merge(df2, df1, how='outer', indicator=True) # Here the unique call is the indicator= True which will add a new columns called as merge to show which are present in which table
      
# Now lets dig the join and join is a method of the data frame, so it can be call as 

import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second dataframe

df.merged = df1.join(df2, rsuffix=right) # This is telling that join df1 with df2 and keep the data frame df2 to the right. 

# In this case, it might be possible that you have the duplicated ids, so to avoid the duplicates ids, you can simply index the dataframes such as 

df.merged = df1.set_inex ('column name').join (df2.set_index('column name')) In this we are indexing the column that you have the same tags and avoiding the duplicates. 

# Append and concatentation fucntions are called as the functions of the dataframe means it will require 2 variables to read append (x,y) or concat (x,y) similar to the join (x,y)
# Append function will not filter anything in the tables, it will simply append the tables, it is similar to the PASTE function of the linux. 

df.append = df1.append(df2, ignore_index=True) 

# Concat is the same as merging two dataframes. Lets start with this 

import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second dataframe

# 1. Both the datsets have unqueal columns so to get all the columns 

df.concat = pd.concat([df1, df2]) <- This will give everything including the duplicated ones. Now to remove the duplicated ones, we can index it. 
df_no_duplicates = df.concat.reset_index(drop=True), you can do this also in the same function like given below: 
df.concat = pd.concat ([df1, df2], ignore_index=TRUE) # concatentate both the indexes and drop the duplicates. 
df.concat = pd.concat([df1, df2], axis= 1) # With axis= 1 we are defining the column on which the concat needs to be performed. 

### Merge and Join use key mapping where as Concat doesnt. Concate works both horizontally and vertically. 

# Updating the table with another table: In this you can use two functions such as combine_first() and update()
import pandas as pd
df1 = pd.read_csv('data.csv') # This is the first dataframe
df2 = pd.read_csv('data.csv') # This is the second datafram

df_new = df1.combine_first(df2) # This will replace the missing values in the df1 from df2. 

df_new = df1.update(df2, overwrite= TRUE) # This will update the values in the df1 from df2 and will overwrite the values.  
df_new = df1.update(df2, overwrite= FALSE) # This will update the values in the df1 from df2 and will not overwrite the values.  

### Making dataframes from PDF files: There is a library for the extraction of the tables from the PDFs and the library is called as tabula
pip install tabula # for the installation of the library. A similar function also present in R : https://cran.r-hub.io/web/packages/tabulizer/vignettes/tabulizer.html

import tabula
myfile = 'filename.pdf'
mytable = tabula.read_pdf(myfile, pages = 1, multiple_tables = TRUE) # Extract the tables from the PDF and keep them as independent tables

import tabula
myfile = 'filename.pdf'
mytable = tabula.read_pdf(myfile, pages = 1, multiple_tables = FALSE) # Extract the tables from the PDF and make a single table
tabula.convert_into(myfile, "file.csv") # export the table into the CSV format

import tabula
myfile = 'filename.pdf'
mytable = tabula.read_pdf(myfile, pages = 1, multiple_tables = FALSE)# Extract the tables from the PDF and make a single table
tabula.convert_into(myfile, "file.csv") # export the table into the CSV format


## Unpacking and contacting multiple files in the panda frame

1. Lets suppose you have multiple files and you want to convert them into the panda frame

a) This will import all the CSV files into the pandas frame, now lets read the code, before that since importing multiple files, is a recursive function, so lets understand the glob function in python, which searches the files recursively


glob function --->  glob.glob(pathname, *, recursive=False), by default the recursive is set to false, means it will not search the files recursively, by you can set the recursive to TRUE then it will match True “**” followed by path separator('./**/').
(geeksforgeerks.org). This is the same as the find function in bash: find . -iname filename (listing all the files in the directory . with the name given in the -iname).

import glob
# Returns a list of names in list files.
print("Using glob.glob()")
files = glob.glob('/home/geeks/Desktop/**/*.txt', recursive = True) # check out the ** which is indicating the recursive true. 
for file in files:
    print(file)
  
# It returns an iterator which will be printed simultaneously.
print("\nUsing glob.iglob()")
for filename in glob.iglob('/home/geeks/Desktop/gfg/**/*.txt',recursive = True):
    print(filename)
    
    
 b) Now if you want to search with the file name or the wildcard like you do in bash 
     $ find . -iname * to list all the files or the $ find . -iname filename, similarly you can list all the files in the directly 
     
     import glob
for name in glob.glob('/home/data/data.txt'): # calling the files with the name tag
    print(name)
  
# Using '*' pattern 
for name in glob.glob('/home/data/*'): # calling all the files in the directory
    print(name)
  
# Using '?' pattern
for name in glob.glob('/home/data/data?.txt'): # calling all the files with the wildcard 
    print(name)
  
# Using [0-9] pattern
for name in glob.glob('/home/data/*[0-9].*'): # calling all the files with the number tag. 
    print(name)
   

Now after finding all the files list, its time to make a dataframe out of it. 

a) Making a data frame from multiple CSV files with in a folder

import glob
import os
import pandas as pd
for name in glob.glob('/home/data/*'): # calling all the files in the directory
    print(name)
all_files = glob.glob("folder/*.csv") 
df = pd.concat((pd.read_csv(f) for f in all_files))
print(df)

b) Making a data from from multiple CSV files from a specified folder, where PATH_TO_FOLDER indicates the PATH. 

home = os.path.expanduser("~")
path = f"{home}PATH_TO_FOLDER"
for name in glob.glob('/home/data/*'): # calling all the files in the directory
    print(name)
all_files = glob.glob(path + "/*.csv")
df = pd.concat((pd.read_csv(f) for f in all_files))
print(df)

c) Making a data frame from the specificed folder which can be a nested folder. 

path = f"{home}PATH_TO_FOLDER"
for name in glob.glob('/home/data/*'): # calling all the files in the directory
    print(name)
all_files = glob.glob(path + "/**/*.csv")
df = pd.concat((pd.read_csv(f) for f in all_files))
print(df)

# To get the path of the foler you can use os.getcwd() and to change the directory you can use os.chdir().

import os
print(os.getcwd())
os.chdir('/Users/name/Documents')
print(os.getcwd())

# Taking into above steps: a complete functional code for the merging panda frame from multiple CVS will be 

import os
print(os.getcwd())
os.chdir('/Users/name/Documents')
print(os.getcwd())
import glob
import os
import pandas as pd
for name in glob.glob('/home/data/*'): # calling all the files in the directory
    print(name)
all_files = glob.glob("folder/*.csv") 
df = pd.concat((pd.read_csv(f) for f in all_files))
print(df)

# To get the summary of the data frame as to see which are the null values and how much and what kind of dataset you have, you can use two commands 
# which will give information and will describe your data frame. 
import pandas as pd
df1 = pd.read_csv('data.csv')
df1.describe()
df1.info()

# Changing the type pattern in the python pandas 

import pandas as pd
df1 = pd.read_csv('data.csv')
df1.describe()
df1.info()
df1[:30] # First thirty rows of the data frame, now see that a slice function has been used to list [start:stop]. 

Basic concept: 
    
    df1 = df1.astype(str) Here i have applied the str type to the entire data series 
    df1 = df1.astype ({'colunname: type'}) Here i have applied to a specific column of the series 
    

type(df1.columnname[]) # so here i am asking the type of the column name in the df1. 
df1.cloumnname = df1.columnname.astype ('init64') # in this you are changing the type of the column

df1 = df.astype({"Column1":'type', "Column2":'type'}) # .astype also accepts dictionary so you can pass multiple columns and data types accordingly.

# Applying pandas.to_numeric, pandas.to_datetime, and pandas.to_timedelta

df1 = df.apply (pd.to_numeric) # this will convert the complete data frame to numeric. However, if you want to apply this to only some columns then make a list of those columns such as
df1 = df [['Columnname1', 'Columnname2']].apply (pd.to_numeric)

# Another way to check the data types is using .dtypes 
import pandas as pd
df1 = pd.read_csv('data.csv')
df1.describe()
df1.info()
df1.dtypes 

and to change the type of the column 

df1 ['Columnname'] = df1['Columname'].astype ('int or str or object or category') 
##### .infer_objects() and .convert_dtypes to PUT. 


 


 













